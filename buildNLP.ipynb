{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10669806-1963-4c97-97d4-cf826f3c9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d40c10-6cce-40c5-97b0-fd117beaa5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intents.json\n",
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Initialize lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',']\n",
    "\n",
    "# Process intents\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50c435-2a77-4a8f-88f3-482070448b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#all the document data will be going to be in the training list which we can use for this neural network\n",
    "\n",
    "# Initialize training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Create training data\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle the training data\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# Separate the features and labels\n",
    "train_x = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))\n",
    "\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad567e4d-c784-4a72-a4d2-878cd0e65ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(train_y[0]), activation='softmax')) # Softmax for multi-class classification\n",
    "\n",
    "# Configure the learning process\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save('chatbot_model.keras', hist)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62aa3db-ea97-4d1c-9a9a-3424cca8aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
